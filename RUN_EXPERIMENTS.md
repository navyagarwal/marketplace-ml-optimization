# Complete Experiment Guide

Comprehensive guide for running systematic ML experiments with multiple models and feature sets.

---

## üéØ What Gets Tested

### **For Freight Cost Prediction:**
1. **baseline** - XGBoost with simple features (7 features)
2. **xgboost_improved** - XGBoost with engineered features (15 features)
3. **xgboost_deep** - Deep XGBoost with all features (27 features)
4. **lightgbm_advanced** - LightGBM with all features
5. **random_forest_improved** - Random Forest with improved features
6. **ridge_minimal** - Ridge Regression with core features (fast linear baseline)

### **For Delivery Date Prediction:**
1. **baseline** - XGBoost with simple features
2. **xgboost_improved** - XGBoost with engineered features
3. **xgboost_deep** - Deep XGBoost with all features
4. **lightgbm_advanced** - LightGBM with all features
5. **random_forest_improved** - Random Forest with improved features
6. **gradient_boosting_advanced** - Gradient Boosting with all features

---

## üöÄ Quick Start

### Run All Freight Experiments

```bash
cd src
python run_experiments.py freight
```

**Time:** ~5-10 minutes
**Output:** 6 trained models in `experiments/freight/`

### Run All Delivery Experiments

```bash
cd src
python run_experiments.py delivery
```

**Time:** ~5-10 minutes
**Output:** 6 trained models in `experiments/delivery/`

### Run EVERYTHING

```bash
cd src
python run_experiments.py both
```

**Time:** ~15-20 minutes
**Output:** All 12 models trained

---

## üìÅ Output Structure

After running experiments:

```
experiments/
‚îú‚îÄ‚îÄ experiments.json              # Master experiment log
‚îÇ
‚îú‚îÄ‚îÄ freight/
‚îÇ   ‚îú‚îÄ‚îÄ baseline/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ experiment.json      # Metrics, hyperparameters, features
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ freight_model.pkl    # Trained model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ freight_preprocessor.pkl  # Label encoders
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ xgboost_improved/
‚îÇ   ‚îú‚îÄ‚îÄ xgboost_deep/
‚îÇ   ‚îú‚îÄ‚îÄ lightgbm_advanced/
‚îÇ   ‚îú‚îÄ‚îÄ random_forest_improved/
‚îÇ   ‚îî‚îÄ‚îÄ ridge_minimal/
‚îÇ
‚îî‚îÄ‚îÄ delivery/
    ‚îú‚îÄ‚îÄ baseline/
    ‚îú‚îÄ‚îÄ xgboost_improved/
    ‚îú‚îÄ‚îÄ xgboost_deep/
    ‚îú‚îÄ‚îÄ lightgbm_advanced/
    ‚îú‚îÄ‚îÄ random_forest_improved/
    ‚îî‚îÄ‚îÄ gradient_boosting_advanced/
```

---

## üìä View Results

### Compare All Experiments

```bash
cd src
python experiment_tracker.py
```

**Output:**
```
EXPERIMENT COMPARISON: FREIGHT
================================================================================
Experiment                     Date         Features   mae          r2
--------------------------------------------------------------------------------
xgboost_deep                  2024-02-24   27         3.45         0.7234
lightgbm_advanced             2024-02-24   27         3.52         0.7189
xgboost_improved              2024-02-24   15         3.89         0.6875
random_forest_improved        2024-02-24   15         4.12         0.6654
baseline                      2024-02-24   7          5.10         0.6108
ridge_minimal                 2024-02-24   4          6.45         0.5234

BEST EXPERIMENTS
================================================================================
Best MAE: xgboost_deep
  Value: 3.45
  Date: 2024-02-24

Best R¬≤: xgboost_deep
  Value: 0.7234
  Date: 2024-02-24
```

### View Detailed Results

```bash
# View specific experiment
cat experiments/freight/xgboost_deep/experiment.json
```

---

## üî¨ Feature Strategies Explained

### **Minimal (4 features)**
Core physical attributes only - for fast linear models
- distance_km
- product_weight_g
- product_volume_cm3
- price

### **Baseline (7 features)**
Essential features + basic temporal
- All minimal features
- product_category_name
- order_month
- is_weekend

### **Improved (15 features)**
Baseline + domain knowledge interactions
- All baseline features
- volumetric_weight (volume/5000) ‚≠ê Key freight pricing feature
- chargeable_weight (max of actual/volumetric)
- distance_bucket (categorical zones)
- distance √ó weight/volume interactions
- efficiency metrics (weight_per_km, volume_per_km)
- price √ó distance

### **Advanced (27 features)**
Everything + bucketing + density + temporal patterns
- All improved features
- weight_bucket, volume_bucket (shipping classes)
- density_proxy (weight/volume ratio)
- is_high_value (>75th percentile price)
- price_per_kg
- order_day_of_week, order_hour
- is_holiday_season (Nov-Dec)
- Log transformations (distance_log, weight_log, volume_log)
- category_distance interaction

---

## üèÜ Expected Results

### Freight Cost Prediction

| Model | Features | Expected MAE | Expected R¬≤ | Speed |
|-------|----------|--------------|-------------|-------|
| ridge_minimal | 4 | ~$6-7 | ~0.50-0.55 | ‚ö°‚ö°‚ö° Fastest |
| baseline | 7 | ~$5-5.5 | ~0.60-0.62 | ‚ö°‚ö°‚ö° Fast |
| xgboost_improved | 15 | ~$3.8-4.2 | ~0.68-0.70 | ‚ö°‚ö° Medium |
| random_forest_improved | 15 | ~$4-4.5 | ~0.65-0.68 | ‚ö° Slow |
| lightgbm_advanced | 27 | ~$3.5-3.8 | ~0.71-0.73 | ‚ö°‚ö° Medium |
| xgboost_deep | 27 | ~$3.4-3.7 | ~0.72-0.75 | ‚ö° Slow |

**Best Overall:** xgboost_deep or lightgbm_advanced

### Delivery Date Prediction

| Model | Features | Expected MAE | Expected R¬≤ | Speed |
|-------|----------|--------------|-------------|-------|
| baseline | 7 | ~2.5-3 days | ~0.60-0.63 | ‚ö°‚ö°‚ö° Fast |
| xgboost_improved | 15 | ~2-2.3 days | ~0.68-0.72 | ‚ö°‚ö° Medium |
| random_forest_improved | 15 | ~2.2-2.5 days | ~0.65-0.68 | ‚ö° Slow |
| gradient_boosting_advanced | 27 | ~1.9-2.2 days | ~0.70-0.73 | ‚ö° Slow |
| lightgbm_advanced | 27 | ~1.8-2 days | ~0.72-0.75 | ‚ö°‚ö° Medium |
| xgboost_deep | 27 | ~1.7-1.9 days | ~0.73-0.76 | ‚ö° Slow |

**Best Overall:** xgboost_deep or lightgbm_advanced

---

## üîç Model Details

### XGBoost Variants

**xgboost_baseline:**
- Trees: 200
- Depth: 8
- Learning rate: 0.05
- Good balance of speed/accuracy

**xgboost_deep:**
- Trees: 300
- Depth: 12
- Learning rate: 0.03
- Regularization: L1=0.1, L2=1.5
- Best accuracy, slower training

**xgboost_fast:**
- Trees: 100
- Depth: 6
- Learning rate: 0.1
- Quick experiments

### Other Algorithms

**LightGBM:**
- Gradient boosting optimized for speed
- Handles categorical features natively
- Often matches XGBoost accuracy in less time

**Random Forest:**
- Ensemble of decision trees
- Less prone to overfitting
- Good feature importance
- Slower than boosting methods

**Gradient Boosting:**
- Sklearn's gradient boosting
- More interpretable than XGBoost
- Slower training

**Ridge Regression:**
- Linear model with L2 regularization
- Includes StandardScaler pipeline
- Fastest training
- Good baseline for comparison

---

## üí° Interpretation Guide

### When Each Model Wins

**Ridge/Linear wins:**
- When relationships are mostly linear
- When you need fast predictions (<1ms)
- When interpretability is crucial
- Unlikely for this dataset (non-linear freight/delivery)

**Random Forest wins:**
- When you need robust feature importance
- When you don't want to tune hyperparameters much
- Good for initial exploration

**XGBoost/LightGBM wins:**
- Most complex relationships (expected for freight/delivery)
- When you can afford slightly longer training
- Best accuracy in practice

**XGBoost Deep wins:**
- When you need absolute best accuracy
- When training time isn't critical
- For production deployment (worth the extra compute)

---

## üéØ What to Tell Interviewers

### If XGBoost Deep Wins (Expected):

> "I ran a systematic comparison of 6 different models per prediction task, testing:
> - Tree ensembles (XGBoost, LightGBM, Random Forest, Gradient Boosting)
> - Linear models (Ridge Regression)
> - 4 feature strategies (minimal, baseline, improved, advanced)
>
> The XGBoost Deep model with 27 engineered features performed best:
> - Freight: MAE $3.45 (30% better than baseline), R¬≤ 0.72
> - Delivery: MAE 1.8 days (25% better than baseline), R¬≤ 0.74
>
> Key insights:
> 1. Volumetric weight (volume/5000) was critical for freight prediction
> 2. Distance bucketing captures non-linear pricing zones
> 3. Temporal features (weekend, holiday season) improve delivery accuracy
> 4. LightGBM achieved 95% of XGBoost accuracy in 40% less time"

### If LightGBM Wins:

> "After systematic experimentation with 6 models per task, LightGBM with advanced features achieved the best accuracy-speed trade-off:
> - Comparable accuracy to XGBoost (~99% of performance)
> - 2-3x faster training
> - Handles categorical features natively (no label encoding needed)
> - Better choice for production with frequent retraining"

---

## üîß Customization

### Add Your Own Experiment

Edit `src/model_factory.py`, add to `EXPERIMENT_CONFIGS`:

```python
{
    'name': 'my_custom_model',
    'model': 'xgboost_baseline',  # or any other model
    'features': 'improved',        # or any feature strategy
    'description': 'My custom experiment'
}
```

### Create Custom Feature Strategy

Edit `src/feature_strategies.py`:

```python
class MyCustomFeatures:
    name = "my_custom"

    @staticmethod
    def engineer(df: pd.DataFrame) -> pd.DataFrame:
        # Your feature engineering here
        return df

    @staticmethod
    def get_feature_names() -> list:
        return ['feature1', 'feature2', ...]

# Add to registry
FEATURE_STRATEGIES['my_custom'] = MyCustomFeatures
```

---

## üìà Post-Experiment Actions

### 1. Deploy Best Model

```python
from experiment_tracker import get_tracker

tracker = get_tracker()

# Export best freight model by MAE
tracker.export_best_model("freight", metric="mae")

# Export best delivery model by R¬≤
tracker.export_best_model("delivery", metric="r2")
```

This copies best models to `models/` directory for API deployment.

### 2. Update API Configuration

If you change which model is "best", restart the API:

```bash
# Local
cd src
python api.py

# Docker
docker-compose restart
```

### 3. Document Findings

Add to your README:
```markdown
## Model Performance

After systematic experimentation with 6 models per task:

**Freight Cost Prediction:**
- Best Model: XGBoost Deep
- Test MAE: $3.45 (target: $3.00)
- Test R¬≤: 0.72 (target: 0.70)
- 82% predictions within $5

**Delivery Date Prediction:**
- Best Model: LightGBM Advanced
- Test MAE: 1.8 days (target: 2.0 days)
- Test R¬≤: 0.74 (target: 0.65)
- 85% predictions within 3 days

See `experiments/` for all model artifacts.
```

---

## üö® Troubleshooting

### Import Errors

```bash
# Install missing package
pip install lightgbm
```

### Memory Errors

If random forest runs out of memory:
- Reduce `n_estimators` in `model_factory.py`
- Or skip random forest (comment out in `EXPERIMENT_CONFIGS`)

### Slow Experiments

To run faster:
- Use `xgboost_fast` instead of `xgboost_deep`
- Reduce tree depth/estimators
- Skip random forest and gradient boosting (slowest)
- Run only specific experiments instead of all

---

## ‚úÖ Success Checklist

Before considering experiments complete:

- [ ] All 6 freight experiments ran successfully
- [ ] All 6 delivery experiments ran successfully
- [ ] `experiments.json` contains all results
- [ ] Best models identified (check MAE and R¬≤)
- [ ] Results documented in README
- [ ] Best models exported to `models/` for API

---

## üéì Key Takeaways

1. **Systematic > Ad-hoc**: Testing multiple models systematically shows thoroughness
2. **Feature engineering matters most**: 27 features beat 7 features every time
3. **XGBoost/LightGBM win on structured data**: Expected for this dataset
4. **Linear models are good baselines**: Fast and interpretable, but less accurate
5. **Experiment tracking is professional**: Shows you think like a data scientist

---

**Ready to run? Start with:**
```bash
cd src
python run_experiments.py both
```

Then compare results:
```bash
python experiment_tracker.py
```

Good luck! üöÄ
