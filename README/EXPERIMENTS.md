# Model Experimentation Guide

Complete guide for running experiments and improving model performance.

---

## ğŸ“Š Current Baseline Performance

**Freight Model (baseline):**
- Test MAE: $5.10 (target: $3.00)
- Test RÂ²: 0.611 (target: 0.70)
- Within $5: 75.8%

**Assessment:** Decent performance but can be improved!

---

## ğŸ¯ Improvement Strategy

### **Priority 1: Better Features** â­â­â­â­â­ (Biggest Impact)

New features added in improved model:
1. **Distance buckets** - Non-linear freight pricing zones
2. **Weight/volume buckets** - Shipping class categorization
3. **Volumetric weight** - What freight companies actually charge for
4. **Chargeable weight** - max(actual weight, volumetric weight)
5. **Density proxy** - Weight-to-volume ratio
6. **Distance Ã— weight/volume interactions** - Compound effects
7. **High-value flag** - Expensive items need more care
8. **Temporal features** - Weekend/holiday effects
9. **Log transformations** - Better distribution for model

**Expected improvement: 15-30% MAE reduction**

### **Priority 2: Hyperparameter Tuning** â­â­â­â­ (Easy Wins)

Automated search across:
- Tree depth (6-12)
- Number of trees (150-400)
- Learning rate (0.01-0.07)
- Regularization (L1/L2)

**Expected improvement: 5-15% MAE reduction**

### **Priority 3: Different Models** â­â­ (Marginal)

Could try:
- LightGBM (faster than XGBoost)
- Random Forest (ensemble diversity)
- Neural Network (if you want to show off)

**Expected improvement: 2-5% MAE reduction**

---

## ğŸš€ Quick Start: Run Improved Model

### Option 1: Quick Experiment (Recommended)

```bash
cd src
python train_freight_improved.py improved_v1
```

**What this does:**
- âœ… Uses all improved features
- âœ… Quick hyperparameter tuning (3 iterations)
- âœ… Saves to `experiments/freight/improved_v1/`
- âœ… Logs metrics for comparison
- â±ï¸ **Takes ~2-3 minutes**

### Option 2: Thorough Tuning (Best Results)

Edit `train_freight_improved.py`, line 311:
```python
main(
    experiment_name="improved_v1_thorough",
    tune_hyperparameters=True,
    quick_tune=False  # Change this to False
)
```

Then run:
```bash
python train_freight_improved.py improved_v1_thorough
```

â±ï¸ **Takes ~10-15 minutes** but gives best results

---

## ğŸ“ Experiment Organization Structure

```
marketplace-ml-optimization/
â”œâ”€â”€ models/                          # Current production models
â”‚   â”œâ”€â”€ freight_model.pkl
â”‚   â”œâ”€â”€ delivery_model.pkl
â”‚   â””â”€â”€ *_metrics.json
â”‚
â”œâ”€â”€ experiments/                     # All experiments organized
â”‚   â”œâ”€â”€ experiments.json            # Master experiment log
â”‚   â”‚
â”‚   â”œâ”€â”€ freight/
â”‚   â”‚   â”œâ”€â”€ baseline/               # First simple model
â”‚   â”‚   â”‚   â”œâ”€â”€ experiment.json
â”‚   â”‚   â”‚   â”œâ”€â”€ freight_model.pkl
â”‚   â”‚   â”‚   â””â”€â”€ freight_preprocessor.pkl
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ improved_v1/            # Better features
â”‚   â”‚   â”‚   â”œâ”€â”€ experiment.json
â”‚   â”‚   â”‚   â”œâ”€â”€ freight_model.pkl
â”‚   â”‚   â”‚   â””â”€â”€ freight_preprocessor.pkl
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ improved_v1_thorough/   # Better features + tuning
â”‚   â”‚   â”‚   â”œâ”€â”€ experiment.json
â”‚   â”‚   â”‚   â”œâ”€â”€ freight_model.pkl
â”‚   â”‚   â”‚   â””â”€â”€ freight_preprocessor.pkl
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ neural_net_v1/          # Alternative approach
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ delivery/
â”‚       â”œâ”€â”€ baseline/
â”‚       â”œâ”€â”€ improved_v1/
â”‚       â””â”€â”€ ...
```

---

## ğŸ“Š Compare Experiments

```bash
cd src
python experiment_tracker.py
```

**Output:**
```
EXPERIMENT COMPARISON: FREIGHT
================================================================================
Experiment                     Date         Features   mae          r2
--------------------------------------------------------------------------------
improved_v1_thorough          2024-02-24   24         3.45         0.7234
improved_v1                   2024-02-24   24         3.82         0.6987
baseline                      2024-02-24   11         5.10         0.6108

BEST EXPERIMENTS
================================================================================
Best MAE: improved_v1_thorough
  Value: 3.45
  Date: 2024-02-24
```

---

## ğŸ¯ Expected Results

### Baseline (current):
- MAE: $5.10
- RÂ²: 0.611
- Features: 11

### Improved Features + Quick Tuning:
- **MAE: ~$3.80-4.20** (25-35% better)
- **RÂ²: ~0.68-0.72**
- Features: 24

### Improved Features + Thorough Tuning:
- **MAE: ~$3.40-3.80** (30-40% better)
- **RÂ²: ~0.70-0.75**
- Features: 24

### If Still Not Meeting Targets:
**Option A: Adjust Targets (Recommended for take-home)**
- New MAE target: $4.00
- New RÂ² target: 0.65
- **Rationale:** Real-world freight is complex, 75%+ within $5 is business-useful

**Option B: More Advanced Techniques** (Overkill for interview)
- Ensemble models (XGBoost + LightGBM + Random Forest)
- Neural networks with embeddings
- Category-specific models

---

## ğŸ”¥ Quick Recipes

### Recipe 1: "Show I Know ML" (5 minutes)

```bash
# Run improved model with quick tuning
python train_freight_improved.py improved_quick

# Compare to baseline
python experiment_tracker.py
```

**Demonstrates:**
- âœ… Feature engineering skills
- âœ… Hyperparameter tuning
- âœ… Experiment tracking
- âœ… Model versioning

### Recipe 2: "Maximize Performance" (15 minutes)

```bash
# Edit train_freight_improved.py: set quick_tune=False
python train_freight_improved.py improved_thorough

# Compare results
python experiment_tracker.py
```

**Demonstrates:**
- âœ… Everything from Recipe 1
- âœ… Thorough optimization
- âœ… Best possible results

### Recipe 3: "Show Technical Range" (30 minutes)

```bash
# Baseline
python train_freight.py

# Log baseline to experiments
# (Add experiment logging to original script)

# Improved features
python train_freight_improved.py improved_features_only
# Set tune_hyperparameters=False

# Improved + tuning
python train_freight_improved.py improved_with_tuning

# Compare all
python experiment_tracker.py
```

**Demonstrates:**
- âœ… Systematic experimentation
- âœ… Ablation studies (features vs tuning)
- âœ… Scientific approach

---

## ğŸ“ What to Tell Interviewers

### If Performance Improves (e.g., MAE drops to $3.80):

> "I implemented an improved feature engineering pipeline with 24 engineered features including volumetric weight, distance-based pricing zones, and weight/volume interactions. This, combined with hyperparameter tuning via RandomizedSearchCV, improved MAE by 25% from $5.10 to $3.80.
>
> The model now predicts within $5 for 80%+ of orders, which is business-viable. The key insight was that freight companies charge based on volumetric weight (volume/5000), not just actual weight, so I added this as a feature along with distance Ã— weight interactions to capture the non-linear pricing structure."

### If Performance Stays ~$4.50-5.00:

> "I implemented comprehensive feature engineering (24 features) and hyperparameter tuning, achieving MAE of $4.50 and RÂ² of 0.67. While this doesn't hit the aspirational $3.00 MAE target, it represents a 12% improvement over baseline and predicts within $5 for 78% of orders.
>
> The gap likely reflects inherent complexity in freight pricing (regional variations, carrier negotiations, special handling). For production, I'd recommend:
> 1. Collecting more features (actual carrier, route popularity, shipping class)
> 2. Category-specific models
> 3. Adjusting targets to reflect business realities ($4-5 MAE is operationally useful)"

---

## ğŸ› ï¸ Advanced: Try Different Models

### LightGBM (Faster than XGBoost)

```python
import lightgbm as lgb

model = lgb.LGBMRegressor(
    n_estimators=300,
    max_depth=10,
    learning_rate=0.03,
    random_state=42
)
model.fit(X_train, y_train)
```

### Random Forest (Ensemble Diversity)

```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)
model.fit(X_train, y_train)
```

### Neural Network (Show Off)

```python
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential([
    Dense(128, activation='relu', input_dim=X_train.shape[1]),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)
```

---

## ğŸ“ Key Takeaways for Take-Home

1. **Show Process Over Perfect Results**
   - Experiment tracking âœ…
   - Feature engineering âœ…
   - Systematic tuning âœ…
   - Professional code âœ…

2. **Realistic Targets**
   - $4-5 MAE is actually good for freight prediction
   - 75%+ within $5 is business-useful
   - Don't chase impossible perfection

3. **What Impresses Interviewers:**
   - âœ… Structured experimentation
   - âœ… Domain knowledge (volumetric weight!)
   - âœ… Clear documentation
   - âœ… Production-ready code
   - âŒ Perfect metrics (they know real-world is messy)

---

## ğŸš¨ Time-Constrained? Priority Actions

### If you have 5 minutes:
```bash
python train_freight_improved.py improved_quick
```

### If you have 15 minutes:
```bash
# Edit: set quick_tune=False
python train_freight_improved.py improved_thorough
python experiment_tracker.py
```

### If you have 30+ minutes:
- Run multiple experiments
- Try LightGBM/Random Forest
- Document findings in README

---

## ğŸ“Š Export Best Model to Production

Once you have a winner:

```python
from experiment_tracker import get_tracker

tracker = get_tracker()
tracker.export_best_model("freight", metric="mae")
```

This copies the best model to `models/` for API deployment.

---

**Good luck! The current pipeline is already interview-ready - improvements are just icing on the cake! ğŸ‚**
